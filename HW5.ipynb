{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3lLobo/basic-probability-programming/blob/master/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4NpTKlJS7v"
      },
      "source": [
        "This week you will implement the EM algorithm. See the assignment for all the details. First we download the data you need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9QhIS8oIDcZ"
      },
      "source": [
        "!wget -O geometric_example_data.txt https://raw.githubusercontent.com/probabll/basic-probability-programming/master/weekly_tasks/week5/homework/geometric_example_data.txt\n",
        "!wget -O geometric_data.txt https://raw.githubusercontent.com/probabll/basic-probability-programming/master/weekly_tasks/week5/homework/geometric_data.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxNO18sWJb8F"
      },
      "source": [
        "Some helper functions that help you working with log-probabilities. You don't need to change this cell. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc73v8NgHuNq"
      },
      "source": [
        "import math\n",
        "\n",
        "def log_add(a, b):\n",
        "    '''Adds to numbers in their logarithmic transformtions.\n",
        "\n",
        "    :param a: The first logarithmically transformed number.\n",
        "    :param b: The second logarithmically transformed number.\n",
        "    :return: The log-sum of the two numbers\n",
        "    '''\n",
        "\n",
        "    if a == -float(\"inf\"):\n",
        "        return b\n",
        "    elif b == -float(\"inf\"):\n",
        "        return a\n",
        "    elif a > b:\n",
        "        return a + math.log1p(math.exp(b-a))\n",
        "    else:\n",
        "        return b + math.log1p(math.exp(a-b))\n",
        "\n",
        "def log_add_list(list_of_numbers):\n",
        "    '''Adds all the logarithmically transformed numbers in a list.\n",
        "\n",
        "    :param list_of_numbers: A list of logarithmically transformed numbers.\n",
        "    '''\n",
        "    result = -float(\"inf\")\n",
        "    for number in list_of_numbers:\n",
        "        result = log_add(number, result)\n",
        "\n",
        "    return result\n",
        "\n",
        "def log_subtract(a , b):\n",
        "    '''Subtracts a logarithmically transformed number b from another such number a.\n",
        "\n",
        "    :param a: The first logarithmically transformed number.\n",
        "    :param b: The second logarithmically transformed number.\n",
        "    :return: The log-difference between a and b\n",
        "    '''\n",
        "\n",
        "    if a == -float(\"inf\"):\n",
        "        return b\n",
        "    elif b == -float(\"inf\"):\n",
        "        return a\n",
        "    elif a > b:\n",
        "        return a + math.log1p(-math.exp(b - a))\n",
        "    else:\n",
        "        return b + math.log1p(-math.exp(a-b))\n",
        "\n",
        "def log_subtract_list(list_of_numbers):\n",
        "    '''Subtracts all the logarithmically transformed numbers in a list from the first one.\n",
        "\n",
        "    :param list_of_numbers: A list of logarithmically transformed numbers.\n",
        "    '''\n",
        "\n",
        "    result = list[0]\n",
        "    for number in list_of_numbers[1:]:\n",
        "        result = log_subtract(result, number)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Num4NJnJuY2"
      },
      "source": [
        "Edit the parts marked with TODO. See the assignment for all the details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofS8RHelH4sI"
      },
      "source": [
        "import random, sys\n",
        "from math import log, exp, factorial\n",
        "# from logarithms import log_add, log_add_list\n",
        "\n",
        "class GeometricDistribution(object):\n",
        "    '''An implementation of the geometric distribution that whose support includes 0.'''\n",
        "\n",
        "    def __init__(self, param=0.5):\n",
        "        '''Constructor\n",
        "\n",
        "        :param param: The parameter of this geometric distribution\n",
        "        :raises: ValueError if param is not in [0,1]\n",
        "        '''\n",
        "\n",
        "        self.failure = log(1 - param)\n",
        "        self.success = log(param)\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        '''Compute the log-probability of an observation\n",
        "\n",
        "        :param x: The observation\n",
        "        :return: The log-probability of x under this distribution\n",
        "        :raises: ValueError if x is not a non-negative integer\n",
        "        '''\n",
        "\n",
        "        if x%1 != 0 or x < 0:\n",
        "            raise ValueError(\"x is not a non-negative integer\")\n",
        "\n",
        "        return self.success + x*self.failure\n",
        "\n",
        "    def get_param(self):\n",
        "        '''Get the parameter of this distribution\n",
        "\n",
        "        :return: The parameter of this distribution\n",
        "        '''\n",
        "        return exp(self.success)\n",
        "\n",
        "\n",
        "class EM(object):\n",
        "    '''A geometric mixture model that can be trained using EM'''\n",
        "\n",
        "    def __init__(self, num_components = 5, initial_mixture_weights = None, initial_geometric_parameters = None):\n",
        "        '''Constructor\n",
        "\n",
        "        :param num_components: The number of mixture components in this model\n",
        "        :param initial_mixture_weights: A list of initial mixture weights (weights are initialised randomly if no list is provided)\n",
        "        :param initial_geometric_parameters: A list of initial component parameters (initialised randomly if no list is provided)\n",
        "        '''\n",
        "\n",
        "        # total number of mixture components\n",
        "        self.num_components = num_components\n",
        "        # map from components to their mixture_weights (stored as logarithms)\n",
        "        self.mixture_weights = list()\n",
        "        # map from components to their distributions\n",
        "        self.component_distributions = list()\n",
        "        # map from components to their expected number of occurrence (stored as logarithms)\n",
        "        self.expected_component_counts = dict()\n",
        "        # map from components to expected values of observations (stored as logarithms)\n",
        "        self.expected_observation_counts = dict()\n",
        "        self.log_likelihood = 0\n",
        "        self.initialise(initial_mixture_weights, initial_geometric_parameters)\n",
        "\n",
        "    def initialise(self, initial_mixture_weights, initial_geometric_parameters):\n",
        "        '''Initialise the parameters of this model\n",
        "\n",
        "        :param initial_mixture_weights: A list of initial mixture weights (weights are initialised randomly if no list is provided)\n",
        "        :param initial_geometric_parameters: A list of initial component parameters (initialised randomly if no list is provided)\n",
        "        '''\n",
        "\n",
        "        #TODO: the following code performs a random initialization of mixture weights and geometric distributions\n",
        "        #TODO: make sure that the input arguments initial_mixtures_weights and initial_geometric_parameters\n",
        "        #TODO: are used if they are actually provided.\n",
        "\n",
        "        for component in range(self.num_components):\n",
        "            self.mixture_weights.append(random.random())\n",
        "            self.component_distributions.append(GeometricDistribution(random.random()))\n",
        "            # values that are stored as logarithms are initialized as -inf = log(0)\n",
        "            self.expected_component_counts[component] = -float(\"inf\")\n",
        "            self.expected_observation_counts[component] = -float(\"inf\")\n",
        "\n",
        "        # normalise priors\n",
        "        prior_sum = sum(self.mixture_weights)\n",
        "        for component in range(self.num_components):\n",
        "            # normalise and tranform to log-prob\n",
        "            self.mixture_weights[component] = log(self.mixture_weights[component] / prior_sum)\n",
        "\n",
        "    def train(self, data_path, iterations):\n",
        "        '''Train the model on data for a fixed number of iterations. After each iteration of training, the log-likelihood,\n",
        "        mixture weights and component parameters are printed out.\n",
        "\n",
        "        :param data_path: The path to the data file\n",
        "        :param iterations: The number of iterations\n",
        "        '''\n",
        "\n",
        "        for iter in range(iterations):\n",
        "            params = list()\n",
        "            priors = list()\n",
        "            for component in range(self.num_components):\n",
        "                params.append(self.component_distributions[component].get_param())\n",
        "                priors.append(exp(self.mixture_weights[component]))\n",
        "\n",
        "            print(\"\\nIteration {}\".format(iter))\n",
        "            print(\"log-likelihood: {}\".format(self.log_likelihood))\n",
        "            print(\"Component priors: {}\".format(priors))\n",
        "            print(\"Component parameters: {}\".format(params))\n",
        "\n",
        "            # reset log-likelihood\n",
        "            self.log_likelihood = 0\n",
        "\n",
        "            self.em(data_path)\n",
        "\n",
        "\n",
        "\n",
        "    def em(self, data_path):\n",
        "        '''Perform one iteration of EM on the data\n",
        "\n",
        "        :param data_path: The path to the data file\n",
        "        '''\n",
        "        with open(data_path) as data:\n",
        "            for line in data:\n",
        "                for observation in line.split():\n",
        "                    self.e_step(int(observation))\n",
        "\n",
        "        self.m_step()\n",
        "\n",
        "\n",
        "    def e_step(self, observation):\n",
        "        '''Perform the E-step on a single obervation. That is, compute the posterior of mixture components\n",
        "        and add the expected occurrence of each component to the running totals\n",
        "        self.log_likelihood ,\n",
        "        self.expected_component_counts , and\n",
        "        self.expected_observation_counts\n",
        "\n",
        "        :param observation: The observation\n",
        "        '''\n",
        "\n",
        "        #TODO: Implement this. Make sure to update the log-likelihood during the E-step.\n",
        "\n",
        "    def m_step(self):\n",
        "        '''Perform the M-step. This step updates\n",
        "        self.mixture_weights\n",
        "        self.component_distributions\n",
        "        '''\n",
        "\n",
        "        # test if the sum of the summed expected_component_counts is roughly equal to the total amount of observations\n",
        "        sum_obs_counts = log_add_list(self.expected_component_counts.values())\n",
        "        print(\"Sum of expected component counts: {}\".format(exp(sum_obs_counts)))\n",
        "\n",
        "\n",
        "        # TODO: Implement this.\n",
        "        # TODO: Make sure to reset the data structures you use for counting after you have\n",
        "        # TODO: updated all parameters, namely self.expected_component_counts and self.expected_observation_counts\n",
        "\n",
        "\n",
        "def main(data_path, number_mixture_components, initial_mixture_weights=None, initial_geometric_parameters=None):\n",
        "    learner = EM(int(number_mixture_components), initial_mixture_weights, initial_geometric_parameters)\n",
        "    learner.train(data_path, 20)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: make sure that the first argument points to the data that you have downloaded\n",
        "    main('geometric_example_data.txt', 2, [0.2, 0.8], [0.2, 0.6])\n",
        "\n",
        "    # TODO: use the following call instead when the small example above is running properly\n",
        "    # main('geometric_data.txt', 3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}