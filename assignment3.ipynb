{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "read_zip.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3lLobo/basic-probability-programming/blob/master/assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5HKdaGR0l9O"
      },
      "source": [
        "Combining wget and unzip, we can easily download a zip file and unpack.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMmDQF2r0BCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5f0ba0-d7ab-432e-f27e-cca7f4a96d8f"
      },
      "source": [
        "!wget -O corpus.zip https://github.com/probabll/basic-probability-programming/raw/master/weekly_tasks/week3/homework/code/corpus.zip\n",
        "!unzip corpus.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-18 10:00:01--  https://github.com/probabll/basic-probability-programming/raw/master/weekly_tasks/week3/homework/code/corpus.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/probabll/basic-probability-programming/master/weekly_tasks/week3/homework/code/corpus.zip [following]\n",
            "--2020-11-18 10:00:01--  https://raw.githubusercontent.com/probabll/basic-probability-programming/master/weekly_tasks/week3/homework/code/corpus.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5559299 (5.3M) [application/zip]\n",
            "Saving to: ‘corpus.zip’\n",
            "\n",
            "corpus.zip          100%[===================>]   5.30M  21.5MB/s    in 0.2s    \n",
            "\n",
            "2020-11-18 10:00:01 (21.5 MB/s) - ‘corpus.zip’ saved [5559299/5559299]\n",
            "\n",
            "Archive:  corpus.zip\n",
            "   creating: corpus/\n",
            "  inflating: corpus/.DS_Store        \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/corpus/\n",
            "  inflating: __MACOSX/corpus/._.DS_Store  \n",
            "  inflating: corpus/1789-Washington.txt  \n",
            "  inflating: corpus/1793-Washington.txt  \n",
            "  inflating: corpus/1797-Adams.txt   \n",
            "  inflating: corpus/1801-Jefferson.txt  \n",
            "  inflating: corpus/1805-Jefferson.txt  \n",
            "  inflating: corpus/1809-Madison.txt  \n",
            "  inflating: corpus/1813-Madison.txt  \n",
            "  inflating: corpus/1817-Monroe.txt  \n",
            "  inflating: corpus/1821-Monroe.txt  \n",
            "  inflating: corpus/1825-Adams.txt   \n",
            "  inflating: corpus/1829-Jackson.txt  \n",
            "  inflating: corpus/1833-Jackson.txt  \n",
            "  inflating: corpus/1837-VanBuren.txt  \n",
            "  inflating: corpus/1841-Harrison.txt  \n",
            "  inflating: corpus/1845-Polk.txt    \n",
            "  inflating: corpus/1849-Taylor.txt  \n",
            "  inflating: corpus/1853-Pierce.txt  \n",
            "  inflating: corpus/1857-Buchanan.txt  \n",
            "  inflating: corpus/1861-Lincoln.txt  \n",
            "  inflating: corpus/1865-Lincoln.txt  \n",
            "  inflating: corpus/1869-Grant.txt   \n",
            "  inflating: corpus/1873-Grant.txt   \n",
            "  inflating: corpus/1877-Hayes.txt   \n",
            "  inflating: corpus/1881-Garfield.txt  \n",
            "  inflating: corpus/1885-Cleveland.txt  \n",
            "  inflating: corpus/1889-Harrison.txt  \n",
            "  inflating: corpus/1893-Cleveland.txt  \n",
            "  inflating: corpus/1897-McKinley.txt  \n",
            "  inflating: corpus/1901-McKinley.txt  \n",
            "  inflating: corpus/1905-Roosevelt.txt  \n",
            "  inflating: corpus/1909-Taft.txt    \n",
            "  inflating: corpus/1913-Wilson.txt  \n",
            "  inflating: corpus/1917-Wilson.txt  \n",
            "  inflating: corpus/1921-Harding.txt  \n",
            "  inflating: corpus/1925-Coolidge.txt  \n",
            "  inflating: corpus/1929-Hoover.txt  \n",
            "  inflating: corpus/1933-Roosevelt.txt  \n",
            "  inflating: corpus/1937-Roosevelt.txt  \n",
            "  inflating: corpus/1941-Roosevelt.txt  \n",
            "  inflating: corpus/1945-Roosevelt.txt  \n",
            "  inflating: corpus/1949-Truman.txt  \n",
            "  inflating: corpus/1953-Eisenhower.txt  \n",
            "  inflating: corpus/1957-Eisenhower.txt  \n",
            "  inflating: corpus/1961-Kennedy.txt  \n",
            "  inflating: corpus/1965-Johnson.txt  \n",
            "  inflating: corpus/1969-Nixon.txt   \n",
            "  inflating: corpus/1973-Nixon.txt   \n",
            "  inflating: corpus/1977-Carter.txt  \n",
            "  inflating: corpus/1981-Reagan.txt  \n",
            "  inflating: corpus/1985-Reagan.txt  \n",
            "  inflating: corpus/1989-Bush.txt    \n",
            "  inflating: corpus/1993-Clinton.txt  \n",
            "  inflating: corpus/1997-Clinton.txt  \n",
            "  inflating: corpus/2001-Bush.txt    \n",
            "  inflating: corpus/2005-Bush.txt    \n",
            "  inflating: corpus/2009-Obama.txt   \n",
            "  inflating: corpus/austen-emma.txt  \n",
            "  inflating: __MACOSX/corpus/._austen-emma.txt  \n",
            "  inflating: corpus/austen-persuasion.txt  \n",
            "  inflating: corpus/austen-sense.txt  \n",
            "  inflating: corpus/bible-kjv.txt    \n",
            "  inflating: corpus/blake-poems.txt  \n",
            "  inflating: corpus/bryant-stories.txt  \n",
            "  inflating: corpus/burgess-busterbrown.txt  \n",
            "  inflating: corpus/carroll-alice.txt  \n",
            "  inflating: corpus/chesterton-ball.txt  \n",
            "  inflating: corpus/chesterton-brown.txt  \n",
            "  inflating: corpus/chesterton-thursday.txt  \n",
            "  inflating: corpus/corpus-subset.json  \n",
            "  inflating: corpus/corpus.json      \n",
            "  inflating: corpus/edgeworth-parents.txt  \n",
            "  inflating: corpus/ep-00-01-17.en   \n",
            "  inflating: corpus/ep-00-01-18.en   \n",
            "  inflating: corpus/ep-00-01-19.en   \n",
            "  inflating: corpus/ep-00-01-21.en   \n",
            "  inflating: corpus/ep-00-02-02.en   \n",
            "  inflating: corpus/ep-00-02-03.en   \n",
            "  inflating: corpus/ep-00-02-14.en   \n",
            "  inflating: corpus/ep-00-02-15.en   \n",
            "  inflating: corpus/ep-00-02-16.en   \n",
            "  inflating: corpus/ep-00-02-17.en   \n",
            "  inflating: corpus/melville-moby_dick.txt  \n",
            "  inflating: corpus/milton-paradise.txt  \n",
            "  inflating: corpus/shakespeare-caesar.txt  \n",
            "  inflating: corpus/shakespeare-hamlet.txt  \n",
            "  inflating: corpus/shakespeare-macbeth.txt  \n",
            "  inflating: corpus/whitman-leaves.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVzjKslc1Pxk"
      },
      "source": [
        "We can now load in the prepared dictionary. See the assignment for the details and purpose of the dictionaries. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_wXnuAv0yEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3313bc4-8746-4cd3-a822-7fd1361568b0"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "corpus = json.load(open('corpus/corpus-subset.json', 'r'))\n",
        "\n",
        "#print an example to inspect the structure of the dictionary. \n",
        "print(corpus[\"1789-Washington\"])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'collection': 'inaugural', 'fid': '1789-Washington', 'filename': 'corpus/1789-Washington.txt'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvzL_d1hzBw_",
        "outputId": "1ca30f45-d000-41c5-9749-1052ce7fa1ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "print_steps = True\n",
        "def step(message):\n",
        "    if print_steps:\n",
        "        print(\"\\n\" + \"*\"*70 + '\\n{:*^70}\\n'.format(message))\n",
        "\n",
        "\n",
        "\n",
        "def split_text(text):\n",
        "    \"\"\"\n",
        "    Split the string in words (tokens)\n",
        "    :param text: string\n",
        "    :return: list\n",
        "    \"\"\"\n",
        "    return text.lower().split(' ')\n",
        "\n",
        "def get_file_freqs(filename):\n",
        "    \"\"\"\n",
        "    Get the word frequencies in a file\n",
        "    :param filename:\n",
        "    :return: Counter\n",
        "    \"\"\"\n",
        "    freqs = Counter()\n",
        "    with open(filename, 'r', encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            words = split_text(line)\n",
        "            freqs.update(words)\n",
        "\n",
        "    return freqs\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "\n",
        "####\n",
        "step(\"1. Collect the corpus frequencies\")\n",
        "# Make a Counter object `corpus_freqs` with the corpus (word) freqs,\n",
        "# the freqs of words in all documents combined. You can use the\n",
        "# function get_file_freqs and Counter.update() might be useful.\n",
        "corpus_freqs = Counter()\n",
        "idf_counter = Counter()\n",
        "for docid, info in corpus.items():\n",
        "    file_freq = get_file_freqs(info['filename'])\n",
        "    corpus_freqs = corpus_freqs + file_freq\n",
        "    idf_counter.update(list(corpus_freqs.keys()))\n",
        "    print('Number of unique words in corpus:', len(corpus_freqs))\n",
        "\n",
        "tf_idf = dict()\n",
        "total_n = len(idf_counter)\n",
        "for key in idf_counter.keys():\n",
        "    tf_idf[key] = corpus_freqs[key] * (np.log(total_n / (1 + idf_counter[key]))+1)\n",
        "\n",
        "####\n",
        "step(\"2. Make vocabulary\")\n",
        "# To scale things down, we only consider some of the most frequent\n",
        "# words in the corpus. Use the method Counter.most_common() to make\n",
        "# a list (!) called `vocabulary` which contains the voc_size=100 most\n",
        "# common words in the corpus\n",
        "voc_size = 100\n",
        "vocabulary = list(zip(*corpus_freqs.most_common(voc_size)))[0]\n",
        "tf_idf_voc = sorted(tf_idf)[:voc_size]\n",
        "\n",
        "####\n",
        "step(\"# 3. Collect vocabulary word frequencies\")\n",
        "# We are only interested in the frequency of words in the vocabulary.\n",
        "# Write a function that, given a frequency counter and the vocabulary\n",
        "# returns a list (!) of frequencies of the words in the vocabulary.\n",
        "# So if freqs['book'] = 10 and 'book' is the 3rd word in the vocabulary,\n",
        "# then the function should output a list with 10 as the 3rd item.\n",
        "def freqs_to_vector(freqs, vocabulary):\n",
        "    \"\"\"\n",
        "    Turns a frequency Counter into a list (!) of frequencies, in the\n",
        "    same order as the words in vocabulary.\n",
        "    :param freqs: a Counter with word frequencies\n",
        "    :param vocabulary: a list of vocabulary words\n",
        "    :return: a list with the frequencies of each of the voc. words\n",
        "    \"\"\"\n",
        "    freq_list = list()\n",
        "    for word in vocabulary:\n",
        "        if word in freqs:\n",
        "            freq_list.append(freqs[word])\n",
        "        else:\n",
        "            freq_list.append(0)\n",
        "    return freq_list\n",
        "\n",
        "\n",
        "####\n",
        "step(\"4. Collect vocabulary word frequencies\")\n",
        "# Store the frequency vector of every document in the `corpus` object\n",
        "# as `corpus[doc_id]['freqs']` (remember `corpus` is a dictionary of\n",
        "# dictionaries). You first have to read out the frequencies again\n",
        "# using `get_file_freqs`, and then you can use `freq_to_vector`.\n",
        "\n",
        "# ...\n",
        "\n",
        "for docid, info in corpus.items():\n",
        "    item_freqs = get_file_freqs(info['filename'])\n",
        "    corpus[docid]['freqs'] = freqs_to_vector(item_freqs, vocabulary)\n",
        "\n",
        "for docid, info in corpus.items():\n",
        "    item_freqs = get_file_freqs(info['filename'])\n",
        "    corpus[docid]['freqs_tfidf'] = freqs_to_vector(item_freqs, tf_idf_voc)\n",
        "####\n",
        "step(\"5. Norm\")\n",
        "# Define a function that returns the norm of a vector (list of numbers).\n",
        "# So e.g. norm([3, 4]) = sqrt( 3^2 + 4^2 ) = 5. You can use the function\n",
        "# `math.sqrt` for the square root, and `sum(my_list)` is also useful\n",
        "\n",
        "import numpy as np\n",
        "def norm(vector):\n",
        "    \"\"\"\n",
        "    Computes the norm of a list of numbers\n",
        "    :param vector: a list of numbers\n",
        "    :return: a number\n",
        "    \"\"\"\n",
        "    return np.linalg.norm(vector)\n",
        "\n",
        "#Here are some tests that your norm function should pass\n",
        "print(\"\\nTest 5: norm:\")\n",
        "print( norm([3, 4]) ) # Should be 5.0\n",
        "print( norm([1, 1, 1, 1])) # Should be 2.0\n",
        "print( norm([5, 20, 102, 9, 1])) # Should be 104.45...\n",
        "\n",
        "####\n",
        "step(\"6. Cosine similarity\")\n",
        "# Write a function that computes the cosine similarity of two vectors\n",
        "# A = [a_1, ..., a_n] and B = [b_1, ..., b_n]. Recall that the cosine\n",
        "# similarity is defined as\n",
        "#   sim = (a_1 * b_1 + ... + a_n * b_n) / ( norm(A) * norm(B) )\n",
        "def similarity(a, b):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity between two vectors (of equal length)\n",
        "    :param A: a vector (list of numbers)\n",
        "    :param B: another vector (list of numbers)\n",
        "    :return: the cosine similarity (a number between -1 and 1)\n",
        "    \"\"\"\n",
        "    if len(a) != len(b):\n",
        "        crop = min(len(a), len(b))\n",
        "        a, b = a[:crop], b[:crop]\n",
        "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "\n",
        "#Here are some tests that your similiary function should pass\n",
        "print(\"\\nTest 6: cosine similarity:\")\n",
        "print( similarity([1,0,0], [1,0,0]) ) # Should be 1.0\n",
        "print( similarity([1,0,0], [0,1,0]) ) # Should be 0.0\n",
        "print( similarity([1,0,0], [-1,0,0]) )# Should be -1.0\n",
        "print( similarity([1,1,.5], [0.5,2,0]))# Should be 0.808...\n",
        "\n",
        "####\n",
        "step(\"7. Compute cosine similarities\")\n",
        "# Compute the cosine similarity between three documents:\n",
        "# the first and second inaugural address by Washington\n",
        "# (id's \"1789-Washington\" and \"1793-Washington\") and the\n",
        "# poetry collection 'Leaves of grass' by Walt Whitman\n",
        "# (id \"whitman-leaves\").\n",
        "washington1 = corpus[\"1789-Washington\"]['freqs']\n",
        "washington2 = corpus[\"1793-Washington\"]['freqs']\n",
        "whitman = corpus['whitman-leaves']['freqs']\n",
        "\n",
        "print(\"\\nCosine similarities\")\n",
        "print(\"Washington 1 vs Washington 2:\", similarity(washington1, washington2))\n",
        "print(\"Washington 1 vs Whitman:\", similarity(washington1, whitman))\n",
        "print(\"Washington 2 vs Whitman:\", similarity(washington2, whitman))\n",
        "\n",
        "\n",
        "####\n",
        "step(\"8. Arbitrary queries\")\n",
        "# We want to use the cosine similarity to automatically find\n",
        "# the document most similar to a 'query', that is, we want\n",
        "# to build a kind of search engine. Write a function that turns\n",
        "# a query string into a frequency vector. You probabl want to use\n",
        "# the functions split_text and freqs_to_vector we defined before.\n",
        "def text_to_vector(text, vocabulary):\n",
        "    \"\"\"\n",
        "    Turns a string into a vector (list) of word frequencies for those\n",
        "     words in the vocabulary\n",
        "    :param text: the input string\n",
        "    :param vocabulary: the list of vocabulary words\n",
        "    :return: a list of word-frequencies\n",
        "    \"\"\"\n",
        "    words_list = split_text(text)\n",
        "    return freqs_to_vector(vocabulary,words_list)\n",
        "\n",
        "\n",
        "# We have already written the function that ranks the documents for you\n",
        "def rank_documents(query_vector, corpus, num=100, tfidf=False):\n",
        "    \"\"\"\n",
        "    Ranks the documents according to their cosine similarity to a query vector.\n",
        "    :param query_vector: list\n",
        "    :param num: only return the `num` top ranking documents\n",
        "    :return: two lists: a list of ranked document ids (most similar first) and a\n",
        "        list with the corresponding similarity scores\n",
        "    \"\"\"\n",
        "    similarities = {}\n",
        "    freq_key = 'freqs_tfidf' if tfidf else 'freqs'\n",
        "    for doc_id, info in corpus.items():\n",
        "        freq_vect = corpus[doc_id][freq_key]\n",
        "        similarities[doc_id] = similarity(query_vector, freq_vect)\n",
        "\n",
        "    ranked_ids = sorted(similarities, key=lambda i: similarities[i], reverse=True)\n",
        "    ranked_sims = [similarities[id] for id in ranked_ids]\n",
        "    return ranked_ids[:num], ranked_sims[:num]\n",
        "\n",
        "\n",
        "####\n",
        "step(\"9. Rank documents\")\n",
        "# Use the functions text_to_vector and rank_documents to find the document\n",
        "# closest to the following excerpt from Adams inaugural address.\n",
        "\n",
        "adams_txt = \"When it was first perceived, in early times, that no middle \\\n",
        "course for America remained between unlimited submission to a foreign \\\n",
        "legislature and a total independence of its claims, men of reflection \\\n",
        "were less apprehensive of danger from the formidable power of fleets \\\n",
        "and armies they must determine to resist than from those contests and \\\n",
        "dissensions which would certainly arise concerning the forms of government \\\n",
        "to be instituted over the whole and over the parts of this extensive country.\"\n",
        "\n",
        "print(rank_documents(text_to_vector(adams_txt, corpus_freqs),corpus))\n",
        "\n",
        "# Do play around with our querying system. To use the full collection,\n",
        "# rather than the 3 corpus we used so far, uncomment the line\n",
        "# `corpus = json.load(open('documents.json', 'r'))` at the top of this file.\n",
        "# You note that our system isn't very reliable, and can be improved in\n",
        "# many ways. The first thing you would want to do is tackle stop-words.\n",
        "#\n",
        "# If you look at the vocabulary (print it!) it contains many words like\n",
        "# 'and', 'of', 'it', and so on. These are highly frequent in all texts,\n",
        "# and not informative for a document's content. There are at least two\n",
        "# improvements: (1) remove such words from the vocabulary, or (2) adjust\n",
        "# our vectors to be less 'sensitive' to those words.\n",
        "#\n",
        "# A common approach to (2) is to use so called tf-idf vectors, which stands\n",
        "# for (term frequency)-(inverse document frequency). The inverse document\n",
        "# frequency roughly punishes words that occur in many of the documents\n",
        "# in the corpus. It is fairly easy to extend this assignment to use\n",
        "# tf-idf scores instead. If you're interested, look up the Wikipedia page\n",
        "# https://en.wikipedia.org/wiki/Tf%E2%80%93idf or ask Bas if you need help.\n",
        "\n",
        "step(\"11. Play around\")\n",
        "print(rank_documents(text_to_vector(adams_txt, corpus_freqs),corpus, tfidf=True))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**********************************************************************\n",
            "******************1. Collect the corpus frequencies*******************\n",
            "\n",
            "Number of unique words in corpus: 634\n",
            "Number of unique words in corpus: 670\n",
            "Number of unique words in corpus: 24164\n",
            "\n",
            "**********************************************************************\n",
            "**************************2. Make vocabulary**************************\n",
            "\n",
            "\n",
            "**********************************************************************\n",
            "***************# 3. Collect vocabulary word frequencies***************\n",
            "\n",
            "\n",
            "**********************************************************************\n",
            "****************4. Collect vocabulary word frequencies****************\n",
            "\n",
            "\n",
            "**********************************************************************\n",
            "*******************************5. Norm********************************\n",
            "\n",
            "\n",
            "Test 5: norm:\n",
            "5.0\n",
            "2.0\n",
            "104.45573225055675\n",
            "\n",
            "**********************************************************************\n",
            "*************************6. Cosine similarity*************************\n",
            "\n",
            "\n",
            "Test 6: cosine similarity:\n",
            "1.0\n",
            "0.0\n",
            "-1.0\n",
            "0.8084520834544433\n",
            "\n",
            "**********************************************************************\n",
            "********************7. Compute cosine similarities********************\n",
            "\n",
            "\n",
            "Cosine similarities\n",
            "Washington 1 vs Washington 2: 0.9635642964253897\n",
            "Washington 1 vs Whitman: 0.9788650627808733\n",
            "Washington 2 vs Whitman: 0.947332958076596\n",
            "\n",
            "**********************************************************************\n",
            "*************************8. Arbitrary queries*************************\n",
            "\n",
            "\n",
            "**********************************************************************\n",
            "**************************9. Rank documents***************************\n",
            "\n",
            "(['1793-Washington', '1789-Washington', 'whitman-leaves'], [0.21585390722429865, 0.13839770743592755, 0.0922351359013132])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}